{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from helper import *\n",
    "import pandas as pd\n",
    "import csv\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_WORDS = 1010000\n",
    "LOGGING_FILE = \"./data/logging.txt\"\n",
    "CURRENT_USER_FILE = \"./data/current_user.txt\"\n",
    "\n",
    "user_profile = \"./data/lang-8-users.csv\"\n",
    "todo_users_file = \"./data/todo_users.txt\"\n",
    "corpus_file = \"./data/paral_sents.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parallel_sents(user_id, journal_id): \n",
    "    \"\"\"Get the original sentences with their corrections, it might be 1-N mapping.\n",
    "    Args:\n",
    "        user_id (str): A string represents user id\n",
    "        journal_id (str): A string represents journal id\n",
    "    Returns:\n",
    "        dictionary: A dictionary which contains original sentence as key and a list of corrections as values\n",
    "    \"\"\"\n",
    "    print(f\"{'*'*20} {journal_id} {'*'*20}\")\n",
    "    url = f\"https://lang-8.com/{user_id}/journals/{journal_id}\"\n",
    "    soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
    "    corrections = get_corrections(soup)\n",
    "    return get_pair_sents(corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_doc_id_pairs(user_profile, total_count=10):\n",
    "    \"\"\"Read the user_id and doc_ids pairs from user_profile. Specify total_count to limit the counts.\n",
    "    Args:\n",
    "        user_profile (str): A filepath to the user profile\n",
    "        total_count(int): Maximum number of <user_id, [journal_ids]> pairs to scrape\n",
    "    Returns:\n",
    "        A generator to serve user_id and its corresponding doc_ids list\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    user_id, document_ids = None, None\n",
    "    with open(user_profile) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            match_user = re.search(r\"<user_id> (.*)\", line)\n",
    "            match_docs = re.search(r\"documents: (\\[.*\\])\", line)\n",
    "            if match_user:\n",
    "                user_id = match_user.group(1)\n",
    "            if match_docs:\n",
    "                document_ids = match_docs.group(1).strip(\"[]\").split(\", \")\n",
    "            if user_id and document_ids:\n",
    "                count += 1\n",
    "                yield user_id, document_ids\n",
    "                user_id, document_ids = None, None\n",
    "                \n",
    "            if count == total_count:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_metadata(output_file, todo_users, done_users_list, total_count=5, mode=\"a\"):\n",
    "    \"\"\"Scrape users from todo_users list and write data to output_file.\n",
    "    Args:\n",
    "        output_file (str): Filepath to store user metadata\n",
    "        todo_users (list): A list which contains todo user_ids\n",
    "        done_users_list (list): A list contains done user_ids\n",
    "        total_count (int): Number of users to scrape\n",
    "        mode (str): The mode to open output_file\n",
    "    Returns:\n",
    "        None (You should expect an ouput file created in your defined path)\n",
    "    \"\"\"\n",
    "    user_count = 0\n",
    "    while todo_users:\n",
    "        current_user = todo_users.pop()\n",
    "        success = False\n",
    "        count = 0\n",
    "        result = {}\n",
    "        print(current_user)\n",
    "        \n",
    "        while not success and count < 10:\n",
    "            try:\n",
    "                # collect metadata of users\n",
    "                friends_page_soup = BeautifulSoup(urlopen(f'https://lang-8.com/{current_user}/friends'), 'html.parser')\n",
    "                documents_page_soup = BeautifulSoup(urlopen(f'https://lang-8.com/{current_user}/journals'), 'html.parser')\n",
    "                profile = get_profile(friends_page_soup)\n",
    "                friends = get_friends(friends_page_soup)\n",
    "                print(\"friends: \" + str(friends))\n",
    "                documents = get_documents(documents_page_soup)\n",
    "                \n",
    "                # write to output_file\n",
    "                with open(output_file, mode=mode, encoding=\"utf-8\") as fout:\n",
    "                    headers = ['Sex', 'Occupation', 'L points', 'ID', 'Nation and region', 'Location', 'Age']\n",
    "                    writer = csv.DictWriter(fout, headers)\n",
    "                    \n",
    "                    write_profile = {k: profile[k] for k in headers if k in profile}\n",
    "                    writer.writerow(write_profile)\n",
    "                \n",
    "                # keep track of current_user\n",
    "                with open(CURRENT_USER_FILE, mode=\"w\", encoding=\"utf-8\") as fout:\n",
    "                    fout.write(\"<user_id> \" + current_user + \"\\n\")\n",
    "                    for k, v in profile.items():\n",
    "                        fout.write(f\"{k}: {v}\\n\")\n",
    "                    fout.write(\"friends: \" + str(friends) + \"\\n\")\n",
    "                    fout.write(\"documents: \" + str(documents) + \"\\n\")\n",
    "                \n",
    "                user_count += 1\n",
    "                success = True\n",
    "                \n",
    "            except:\n",
    "                print(count, \" fail!\")\n",
    "                traceback.print_exc()\n",
    "                count += 1\n",
    "                time.sleep(1)\n",
    "        \n",
    "        # max number of tries to make if error happens\n",
    "        if count == 10:\n",
    "            continue\n",
    "        \n",
    "        # update the todo users list\n",
    "        for friend in friends:\n",
    "            if friend not in done_users_list:\n",
    "                todo_users.add(friend)\n",
    "        \n",
    "        fout = open(todo_users_file, \"w\")\n",
    "        json.dump(list(todo_users),fout)\n",
    "        fout.close()\n",
    "        \n",
    "        # update done users list\n",
    "        done_users_list.add(current_user)\n",
    "        \n",
    "        # if reaches max number of users to scrape, stop\n",
    "        if user_count == total_count:\n",
    "            break\n",
    "            \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_parralel_sents(user_profile, pral_corpus, total_count=100, word_count=0):\n",
    "    \"\"\"Scrape parallel sentences from user_profile. Specify total_count to limit counts.\n",
    "    Args:\n",
    "        user_profile (str): A filepath of users profile\n",
    "        paral_corpus (str): A filepath to write the parallel sentences\n",
    "        total_count (int): Number of journals to scrape\n",
    "        word_count (int): Total word counts to keep track of\n",
    "    Returns:\n",
    "        int: Cumulative word count since scraping\n",
    "    \"\"\"\n",
    "    doc_count = 0\n",
    "    for user_id, doc_ids in read_user_doc_id_pairs(user_profile, total_count):\n",
    "        for journal_id in doc_ids:\n",
    "            fail_count = 0\n",
    "            success = False\n",
    "            \n",
    "            while not success and fail_count < 10:\n",
    "                try:\n",
    "                    \n",
    "                    journal_id = journal_id.strip(\"'\")\n",
    "                    if not journal_id:\n",
    "                        break\n",
    "                        \n",
    "                    # get parallel sents from user_id and journal_id pair\n",
    "                    sent_pairs = download_parallel_sents(user_id=user_id, journal_id=journal_id)\n",
    "                    print(f\"Now scraping: {user_id}, {journal_id}\")\n",
    "                    \n",
    "                    # scraping parallel sents\n",
    "                    with open(pral_corpus, 'a', encoding=\"utf-8\") as fout:\n",
    "                        headers = [\"user_id\", \"journal_id\", \"original\", \"corrected\"]\n",
    "                        writer = csv.DictWriter(fout, headers)\n",
    "                        for k, v in sent_pairs.items():\n",
    "                            \n",
    "                            # check if the sentence only ontains English chars\n",
    "                            if re.match(r\"^[a-zA-Z0-9. -_?]*$\", k):\n",
    "                                for i in range(len(v)):\n",
    "                                    if re.match(r\"^[a-zA-Z0-9. -_?]*$\", v[i]):\n",
    "                                        row = {\"user_id\": user_id, \"journal_id\": journal_id, \"original\": k, \"corrected\": v[i]}\n",
    "                                        writer.writerow(row)\n",
    "                                        word_count += len(k.split())\n",
    "                                        print(f\"org: {k}\")\n",
    "                                        print(word_count)\n",
    "                            \n",
    "                            # if reached the total number of tokens, stop\n",
    "                            if word_count >= TOTAL_WORDS:\n",
    "                                return word_count\n",
    "                    doc_count += 1\n",
    "                    success = True\n",
    "                    \n",
    "                except:\n",
    "                    print(fail_count, \"fail\")\n",
    "                    traceback.print_exc()\n",
    "                    fail_count += 1\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # max number of tries to make when error occurs\n",
    "                    if fail_count == 10:\n",
    "                        continue\n",
    "               \n",
    "            # if reached max number of journal counts, stop\n",
    "            if doc_count == total_count:\n",
    "                return word_count\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this section to check POC, scrape one user profile ###\n",
    "\n",
    "# test_user_id = \"191274\"\n",
    "# test_doc_id = \"271352583976235145865979226599667907075\"\n",
    "# user_link = \"https://lang-8.com/191274/friends\"\n",
    "# journal_link = \"https://lang-8.com/191274/journals/271352583976235145865979226599667907075\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this section to check POC, scrape one user profile ###\n",
    "\n",
    "# scrape_metadata(\"./data/test_user_meta.txt\", set([test_user_id]), done_users_list=set(), total_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this section to check POC, scrape one document with parallel sents ###\n",
    "\n",
    "# scrape_parralel_sents(\"./data/test_user_meta.txt\", \"./data/test_paral_sents.txt\", total_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this part to scrape whole corpus\n",
    "\n",
    "word_count = 0\n",
    "\n",
    "todo_users = set([\"213725\",\"673707\"])\n",
    "done_users_list = set()\n",
    "\n",
    "# Check how many words have been scraped\n",
    "if os.path.exists(LOGGING_FILE):\n",
    "    f = open(LOGGING_FILE)\n",
    "    word_count = int(f.read().split(\":\")[-1].strip())\n",
    "    f.close()\n",
    "print(f\"cumulative word count: {word_count}\")\n",
    "\n",
    "# Load the done users list\n",
    "if os.path.exists(user_profile):\n",
    "    f = open(user_profile, encoding='utf-8')\n",
    "    for line in f:\n",
    "        matched = re.search(r\"<user_id> (.*)\", line.strip())\n",
    "        if matched:\n",
    "            done_users_list.add(matched.group(1))\n",
    "    f.close()\n",
    "print(f\"Done users list: {done_users_list}\")\n",
    "  \n",
    "# load the todo users list\n",
    "if os.path.exists(todo_users_file):\n",
    "    f = open(todo_users_file,encoding=\"utf-8\")\n",
    "    todo_users = set(json.load(f))\n",
    "    f.close()\n",
    "print(f\"todo users list: {todo_users}\")\n",
    "\n",
    "# Scrape the whole corpus with TOTAL_WORDS\n",
    "while word_count < TOTAL_WORDS:\n",
    "    \n",
    "    scrape_metadata(user_profile, todo_users, done_users_list, total_count=1, mode=\"a+\")\n",
    "    count = scrape_parralel_sents(CURRENT_USER_FILE, corpus_file, word_count=word_count)\n",
    "    \n",
    "    word_count = count\n",
    "    print(f\"Current count is: {word_count}\")\n",
    "    \n",
    "    # Keep track of total words count\n",
    "    with open(LOGGING_FILE, 'w') as fout:\n",
    "        fout.write(f\"Number of words collected: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel sents:  (77442, 4)\n",
      "User metadata:  (3236, 7)\n"
     ]
    }
   ],
   "source": [
    "parl_sents_df = pd.read_csv(\n",
    "    corpus_file,\n",
    "    names=[\"user_id\", \"doc_id\", \"original\", \"corrected\"],\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "user_df = pd.read_csv(\n",
    "    user_profile,\n",
    "    names=[\n",
    "        \"sex\",\n",
    "        \"occupation\",\n",
    "        \"lpoints\",\n",
    "        \"user_id\",\n",
    "        \"nation_region\",\n",
    "        \"location\",\n",
    "        \"age\",\n",
    "    ],\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "print(\"Parallel sents: \", parl_sents_df.shape)\n",
    "print(\"User metadata: \", user_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>original</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>673707</td>\n",
       "      <td>132180776574210182836253618202132330403</td>\n",
       "      <td>Sun room</td>\n",
       "      <td>Sunroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>673707</td>\n",
       "      <td>132180776574210182836253618202132330403</td>\n",
       "      <td>I've always wanted a sun room for a long time.</td>\n",
       "      <td>I've always wanted a sunroom for a long time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>673707</td>\n",
       "      <td>132180776574210182836253618202132330403</td>\n",
       "      <td>It's because it can dry laundry even if it rai...</td>\n",
       "      <td>It's because I can dry the laundry there even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>673707</td>\n",
       "      <td>132180776574210182836253618202132330403</td>\n",
       "      <td>This time, our city will subsidize 10% of reno...</td>\n",
       "      <td>This time, our city will subsidize 10% of reno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>673707</td>\n",
       "      <td>132180776574210182836253618202132330403</td>\n",
       "      <td>I'm going to make a big purchase, so I want to...</td>\n",
       "      <td>I'm going to make a big purchase, so I want to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                   doc_id  \\\n",
       "0  673707  132180776574210182836253618202132330403   \n",
       "1  673707  132180776574210182836253618202132330403   \n",
       "2  673707  132180776574210182836253618202132330403   \n",
       "3  673707  132180776574210182836253618202132330403   \n",
       "4  673707  132180776574210182836253618202132330403   \n",
       "\n",
       "                                            original  \\\n",
       "0                                           Sun room   \n",
       "1     I've always wanted a sun room for a long time.   \n",
       "2  It's because it can dry laundry even if it rai...   \n",
       "3  This time, our city will subsidize 10% of reno...   \n",
       "4  I'm going to make a big purchase, so I want to...   \n",
       "\n",
       "                                           corrected  \n",
       "0                                            Sunroom  \n",
       "1      I've always wanted a sunroom for a long time.  \n",
       "2  It's because I can dry the laundry there even ...  \n",
       "3  This time, our city will subsidize 10% of reno...  \n",
       "4  I'm going to make a big purchase, so I want to...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parl_sents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>occupation</th>\n",
       "      <th>lpoints</th>\n",
       "      <th>user_id</th>\n",
       "      <th>nation_region</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>Housewife/ Househusband</td>\n",
       "      <td>32,780</td>\n",
       "      <td>673707.0</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85,440</td>\n",
       "      <td>876909.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62,920</td>\n",
       "      <td>579569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>1698991.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>615,110</td>\n",
       "      <td>1406282.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex               occupation  lpoints    user_id nation_region location  \\\n",
       "0  Female  Housewife/ Househusband   32,780   673707.0         Japan    Japan   \n",
       "1     NaN                      NaN   85,440   876909.0           NaN      NaN   \n",
       "2     NaN                      NaN   62,920   579569.0           NaN      NaN   \n",
       "3     NaN                      NaN      250  1698991.0           NaN      NaN   \n",
       "4     NaN                      NaN  615,110  1406282.0           NaN      NaN   \n",
       "\n",
       "    age  \n",
       "0  39.0  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel sents:  (76866, 4)\n"
     ]
    }
   ],
   "source": [
    "# drop NANs\n",
    "parl_sents_df = parl_sents_df.dropna(subset=[\"original\", \"corrected\"], how=\"any\")\n",
    "print(\"Parallel sents: \", parl_sents_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel sents:  (75122, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates and sentence shorter than 4 tokens\n",
    "parl_sents_df = remove_duplicates(parl_sents_df, [\"original\", \"corrected\"])\n",
    "parl_sents_df = remove_n_less_sents(parl_sents_df)\n",
    "print(\"Parallel sents: \", parl_sents_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 988986\n",
      "Number of users: 1519\n",
      "Number of jorunals: 10565\n"
     ]
    }
   ],
   "source": [
    "# calculate number of tokens in the corpus(original sents)\n",
    "total_tokens = sum(parl_sents_df[\"original\"].str.split().str.len())\n",
    "total_users = len(parl_sents_df[\"user_id\"].unique())\n",
    "total_docs = len(parl_sents_df[\"doc_id\"].unique())\n",
    "\n",
    "print(f\"Corpus size: {total_tokens}\")\n",
    "print(f\"Number of users: {total_users}\")\n",
    "print(f\"Number of jorunals: {total_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parl_sents_df.to_csv(\"./data/clean_paral_sents.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
