{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CORPUS = pd.read_csv(\"data/final_corpus.csv\", encoding=\"utf-8\").set_index(\"id\")\n",
    "METADATA = pd.read_csv(\n",
    "    \"data/lang-8-users.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    dtype={\"user_id\": str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_MAPS = {\n",
    "    0: \"Japan\", \n",
    "    1: \"China\",\n",
    "    2: \"Taiwan\",\n",
    "    3: \"Vietnam\",\n",
    "    4: \"Russia\",\n",
    "    5: \"Korea\",\n",
    "    6: \"Brazil\",\n",
    "    7: \"U.S.A\",\n",
    "    8: \"France\",\n",
    "    9: \"Spain\"\n",
    "}\n",
    "\n",
    "OCCUPATION_MAPS = {\n",
    "    0: \"Artist\",\n",
    "    1: \"Designer\",\n",
    "    2: \"Engineer\",\n",
    "    3: \"Housewife/ Househusband\",\n",
    "    4: \"Office worker\",\n",
    "    5: \"Programmer\",\n",
    "    6: \"Student\",\n",
    "    7: \"Teacher\",\n",
    "    8: \"Other\"\n",
    "}\n",
    "\n",
    "GENDER_MAPS = {\n",
    "    0: \"Female\",\n",
    "    1: \"Male\"\n",
    "}\n",
    "\n",
    "SENT_TYPE_MAPS = {\n",
    "    0: \"original\",\n",
    "    1: \"corrected\"\n",
    "}\n",
    "\n",
    "METADATA_KEYS = {\"sex\", \"occupation\", \"location\", \"Lpoints\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_metadata_dict(param_dict):\n",
    "    \"\"\"Construct the metadata dictionary based on param_dict provided\"\"\"\n",
    "    new_dict = {}\n",
<<<<<<< HEAD
    "\n",
=======
    "    \n",
>>>>>>> 8dc2bb8... backend notebook
    "    for k, v in param_dict.items():\n",
    "        if k in METADATA_KEYS:\n",
    "            if v == -1:\n",
    "                continue\n",
    "            if k == \"sex\":\n",
    "                if v in GENDER_MAPS:\n",
    "                    v = GENDER_MAPS[v]\n",
    "            elif k == \"occupation\":\n",
    "                if v in OCCUPATION_MAPS:\n",
    "                    v = OCCUPATION_MAPS[v]\n",
    "            elif k == \"location\":\n",
    "                if v in LOCATION_MAPS:\n",
    "                    v = LOCATION_MAPS[v]\n",
<<<<<<< HEAD
    "            new_dict[k] = v\n",
=======
    "            new_dict[k] = v \n",
>>>>>>> 8dc2bb8... backend notebook
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_range(corpus, sent_type, num_range=(10, 15)):\n",
<<<<<<< HEAD
    "    \"\"\"Returns sentence pairs with only sentence length within num_range, sent_type\n",
=======
    "    \"\"\"Returns sentence pairs with only sentence length within num_range, sent_type \n",
>>>>>>> 8dc2bb8... backend notebook
    "    is used to specify whether the restrictions are put on original or corrected sentences\n",
    "    \"\"\"\n",
    "    lower, upper = num_range\n",
    "    if sent_type in SENT_TYPE_MAPS:\n",
    "        mask = (corpus[SENT_TYPE_MAPS[sent_type]].str.len() >= lower) & (\n",
    "            corpus[SENT_TYPE_MAPS[sent_type]].str.len() <= upper\n",
    "        )\n",
    "        return corpus.loc[mask]\n",
    "    else:\n",
    "        org_mask = (corpus[\"original\"].str.len() >= lower) & (\n",
    "            corpus[\"original\"].str.len() <= upper\n",
    "        )\n",
    "        cor_mask = (corpus[\"corrected\"].str.len() >= lower) & (\n",
    "            corpus[\"corrected\"].str.len() <= upper\n",
    "        )\n",
    "        mask = org_mask & cor_mask\n",
    "        return corpus.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_multiple_corrections(corpus, n_corrections):\n",
    "    \"\"\"Return the sentences which only contains n number of corrections\"\"\"\n",
    "    columns = CORPUS.columns.to_list()\n",
    "    grouped = corpus.groupby(\"original\").agg(\"count\").reset_index()\n",
    "    filtered = grouped[grouped[\"corrected\"] >= n_corrections]\n",
    "    return pd.merge(filtered, CORPUS, on=\"original\", suffixes=(\"_y\", \"\"))[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def get_by_metadata(corpus, **args):\n",
=======
    "def get_by_metadata(corpus, **args): \n",
>>>>>>> 8dc2bb8... backend notebook
    "    \"\"\"Filter the corpus based on the args provided\"\"\"\n",
    "    if not len(args):\n",
    "        return corpus\n",
    "    query_str = \" & \".join([f\"{k} == '{v}'\" for k, v in args.items() if k != \"Lpoints\"])\n",
    "    print(query_str)\n",
    "    filtered = METADATA\n",
    "    if query_str:\n",
    "        filtered = METADATA.query(query_str)\n",
    "    if \"Lpoints\" in args and args[\"Lpoints\"] != \"all\":\n",
    "        query_str = f\"lpoints >= {args['Lpoints'][0]} & lpoints <= {args['Lpoints'][1]}\"\n",
    "        print(query_str)\n",
    "        filtered = filtered.query(query_str)\n",
<<<<<<< HEAD
    "    user_lst = filtered[\"user_id\"].to_list()\n",
=======
    "    user_lst = filtered['user_id'].to_list()\n",
>>>>>>> 8dc2bb8... backend notebook
    "    return corpus.query(\"user_id in @user_lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_corpus(corpus, size=10):\n",
    "    \"\"\"Return a part of the corpus based on the size\"\"\"\n",
    "    return corpus[:size].T.to_dict(\"list\")"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(df, str_sentence):\n",
    "    return df[str_sentence].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pair_set(df):\n",
    "    original_list = extract_list(df, 'original')\n",
    "    corrected_list = extract_list(df, 'corrected')\n",
    "    sentence_pair_set = {(original_list[i], corrected_list[i]) for i in range(len(original_list))}\n",
    "    return sentence_pair_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_search(\n",
    "    final_corpus, agreementType, wordRange, nCorrections, sentenceType\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes as user input and returns the set of pairs (original sentence, corrected sentence).\n",
    "    --------------------------\n",
    "    Argument: final_corpus (final corpus in pandas data frame form)\n",
    "              agreementType (Number of annotators who agreed with the correction - integer between 0 and 3)\n",
    "              wordRange (Number of words in a sentence)\n",
    "              nCorrections (number of corrections offered in the website)\n",
    "              sentenceType (original / correct / both)\n",
    "    Returns: a set of pairs (original sentence, corrected sentence) which meet the requirements specified\n",
    "    \"\"\"\n",
    "    take_intersection = []  # contains the set of sentence pairs to be intersected\n",
    "    if (\n",
    "        agreementType > 3\n",
    "        or agreementType < -1\n",
    "        or (nCorrections != -1 and nCorrections < 1)\n",
    "        or sentenceType > 1\n",
    "        or sentenceType < -1\n",
    "        or type(sentenceType) != int\n",
    "    ):\n",
    "        return set()  # return an empty set for invalid requirements\n",
    "    elif agreementType >= 0 and agreementType <= 3:\n",
    "        df = final_corpus[final_corpus[\"total_agree\"] == agreementType][\n",
    "            [\"original\", \"corrected\"]\n",
    "        ]\n",
    "        take_intersection.append(extract_pair_set(df))\n",
    "    word_min = 0\n",
    "    word_max = 100000\n",
    "    str_list = []\n",
    "    if wordRange != -1:\n",
    "        word_min, word_max = wordRange\n",
    "    if sentenceType == 0 or sentenceType == -1:\n",
    "        str_list.append(\"original\")\n",
    "    elif sentenceType == 1:\n",
    "        str_list.append(\"corrected\")\n",
    "    if sentenceType == -1:\n",
    "        str_list.append(\"corrected\")\n",
    "    for str_sentenceType in str_list:\n",
    "        df_min = pd.DataFrame(\n",
    "            final_corpus[\n",
    "                final_corpus[str_sentenceType].apply(lambda x: len(x.split()))\n",
    "                >= word_min\n",
    "            ][[\"original\", \"corrected\"]]\n",
    "        )\n",
    "        df_max = pd.DataFrame(\n",
    "            final_corpus[\n",
    "                final_corpus[str_sentenceType].apply(lambda x: len(x.split()))\n",
    "                <= word_max\n",
    "            ][[\"original\", \"corrected\"]]\n",
    "        )\n",
    "        take_intersection.append(extract_pair_set(df_min))\n",
    "        take_intersection.append(extract_pair_set(df_max))\n",
    "    if nCorrections > 0:\n",
    "        df = final_corpus.groupby(\"original\").nunique()\n",
    "        original = set(df[df[\"corrected\"] == nCorrections].index.tolist())\n",
    "        original_df = pd.DataFrame(original, columns=[\"original\"])\n",
    "        original_count = pd.merge(\n",
    "            original_df, final_corpus, how=\"left\", on=[\"original\"]\n",
    "        )\n",
    "        take_intersection.append(extract_pair_set(original_count))\n",
    "    if len(take_intersection) == 0:\n",
    "        return final_corpus\n",
    "    else:\n",
    "        ss = take_intersection[0]\n",
    "        for i in range(1, len(take_intersection)):\n",
    "            ss = ss & take_intersection[i]\n",
    "    return ss  # a set of pairs (original, corrected)"
>>>>>>> 8dc2bb8... backend notebook
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(df, str_sentence):\n",
    "    return df[str_sentence].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pair_set(df):\n",
    "    original_list = extract_list(df, \"original\")\n",
    "    corrected_list = extract_list(df, \"corrected\")\n",
    "    sentence_pair_set = {\n",
    "        (original_list[i], corrected_list[i]) for i in range(len(original_list))\n",
    "    }\n",
    "    return sentence_pair_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_search(\n",
    "    final_corpus, agreementType, wordRange, nCorrections, sentenceType\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes as user input and returns the set of pairs (original sentence, corrected sentence).\n",
    "    --------------------------\n",
    "    Argument: final_corpus (final corpus in pandas data frame form)\n",
    "              agreementType (Number of annotators who agreed with the correction - integer between 0 and 3)\n",
    "              wordRange (Number of words in a sentence)\n",
    "              nCorrections (number of corrections offered in the website)\n",
    "              sentenceType (original / correct / both)\n",
    "    Returns: a set of pairs (original sentence, corrected sentence) which meet the requirements specified\n",
    "    \"\"\"\n",
    "    take_intersection = []  # contains the set of sentence pairs to be intersected\n",
    "    if (\n",
    "        agreementType > 3\n",
    "        or agreementType < -1\n",
    "        or (nCorrections != -1 and nCorrections < 1)\n",
    "        or sentenceType > 1\n",
    "        or sentenceType < -1\n",
    "        or type(sentenceType) != int\n",
    "    ):\n",
    "        return set()  # return an empty set for invalid requirements\n",
    "    elif agreementType >= 0 and agreementType <= 3:\n",
    "        df = final_corpus[final_corpus[\"total_agree\"] == agreementType][\n",
    "            [\"original\", \"corrected\"]\n",
    "        ]\n",
    "        take_intersection.append(extract_pair_set(df))\n",
    "    word_min = 0\n",
    "    word_max = 100000\n",
    "    str_list = []\n",
    "    if wordRange != -1:\n",
    "        word_min, word_max = wordRange\n",
    "    if sentenceType == 0 or sentenceType == -1:\n",
    "        str_list.append(\"original\")\n",
    "    elif sentenceType == 1:\n",
    "        str_list.append(\"corrected\")\n",
    "    if sentenceType == -1:\n",
    "        str_list.append(\"corrected\")\n",
    "    for str_sentenceType in str_list:\n",
    "        df_min = pd.DataFrame(\n",
    "            final_corpus[\n",
    "                final_corpus[str_sentenceType].apply(lambda x: len(x.split()))\n",
    "                >= word_min\n",
    "            ][[\"original\", \"corrected\"]]\n",
    "        )\n",
    "        df_max = pd.DataFrame(\n",
    "            final_corpus[\n",
    "                final_corpus[str_sentenceType].apply(lambda x: len(x.split()))\n",
    "                <= word_max\n",
    "            ][[\"original\", \"corrected\"]]\n",
    "        )\n",
    "        take_intersection.append(extract_pair_set(df_min))\n",
    "        take_intersection.append(extract_pair_set(df_max))\n",
    "    if nCorrections > 0:\n",
    "        df = final_corpus.groupby(\"original\").nunique()\n",
    "        original = set(df[df[\"corrected\"] == nCorrections].index.tolist())\n",
    "        original_df = pd.DataFrame(original, columns=[\"original\"])\n",
    "        original_count = pd.merge(\n",
    "            original_df, final_corpus, how=\"left\", on=[\"original\"]\n",
    "        )\n",
    "        take_intersection.append(extract_pair_set(original_count))\n",
    "    if len(take_intersection) == 0:\n",
    "        return extract_pair_set(final_corpus)\n",
    "    else:\n",
    "        ss = take_intersection[0]\n",
    "        for i in range(1, len(take_intersection)):\n",
    "            ss = ss & take_intersection[i]\n",
    "    return ss  # a set of pairs (original, corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_sents(args):\n",
    "\n",
    "    # Targeted corpus\n",
    "    metadata_params = construct_metadata_dict(args)\n",
    "    corpus = get_by_metadata(CORPUS, **metadata_params)\n",
    "    res = corpus\n",
    "\n",
    "    if args[\"corpusType\"] == 1:\n",
    "        res = res[~res[\"agree\"].isna()]\n",
    "        # filtered by agreementType\n",
    "        res = annotation_search(\n",
    "            res,\n",
    "            args[\"agreementType\"],\n",
    "            args[\"wordRange\"],\n",
    "            args[\"nCorrections\"],\n",
    "            args[\"sentenceType\"],\n",
    "        )\n",
    "        return res\n",
    "    elif args[\"corpusType\"] == 0:\n",
    "        res = res[res[\"agree\"].isna()]\n",
    "    else:\n",
    "        res = res\n",
    "    if args[\"wordRange\"] != -1:\n",
    "        res = get_by_range(res, args[\"sentenceType\"], args[\"wordRange\"])\n",
    "    if args[\"nCorrections\"] != -1:\n",
    "        res = get_by_multiple_corrections(res, args[\"nCorrections\"])[\n",
    "            [\"original\", \"corrected\"]\n",
    "        ]\n",
    "    return extract_pair_set(res)"
=======
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75122"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_search(CORPUS, -1, -1, -1, -1))"
>>>>>>> 8dc2bb8... backend notebook
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_sents(args):\n",
    "    \n",
    "    # Targeted corpus\n",
    "    metadata_params = construct_metadata_dict(args)\n",
    "    corpus = get_by_metadata(CORPUS, **metadata_params)\n",
    "    res = corpus\n",
    "    if args[\"corpusType\"] == 1:\n",
    "        res = res[~res[\"agree\"].isna()]\n",
    "        # filtered by agreementType\n",
    "        res = annotation_search(\n",
    "            res,\n",
    "            args[\"agreementType\"],\n",
    "            args[\"wordRange\"],\n",
    "            args[\"nCorrections\"],\n",
    "            args[\"sentenceType\"],\n",
    "        )\n",
    "        return res\n",
    "    elif args[\"corpusType\"] == 0:\n",
    "        res = res[res[\"agree\"].isna()]\n",
    "    else:\n",
    "        res = res\n",
    "    res = get_by_range(res, args[\"sentenceType\"], args[\"wordRange\"])\n",
    "    res = get_by_multiple_corrections(res, args[\"nCorrections\"])[[\"original\", \"corrected\"]]\n",
    "    return extract_pair_set(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 8dc2bb8... backend notebook
    "sample = {\n",
    "   \"corpusType\": 1,  #0-unannotated, 1-annotated, -1 -both (any)\n",
    "   \"agreementType\": -1, #0 = 3 no's, 1 = 1 yes, 2 no's, 2 = 2 yeses 1 no, 3 = 3 yeses (if agreementType=-1, means any agreement type)\n",
    "   \"sentenceType\": -1, #0-original, 1-corrected, -1 -both (any)\n",
    "   \"wordRange\": -1, #based on 'sentenceType', sentences with word range in 10-20 inclusively (if wordRange = -1, means any word range)\n",
    "   \"nCorrections\": -1, #sentences with n corrections (-1 means any)\n",
    "   \"sex\": -1, #0-male, 1-female, 2-other (-1 means any)\n",
    "   \"occupation\": -1, #should we have a list of fixed occupations or string is ok? (\"all\" means all :) )\n",
    "   \"location\": -1, #should we have a list of fixed locations or string is ok? (\"all\" means all :) )\n",
    "   \"Lpoints\": -1 #Lpoints in the given range inclusively (\"all\" means any)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 101,
>>>>>>> 8dc2bb8... backend notebook
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
<<<<<<< HEAD
     "execution_count": 22,
=======
     "execution_count": 101,
>>>>>>> 8dc2bb8... backend notebook
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_target_sents(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Japan': 17677,\n",
       " 'China': 5354,\n",
       " 'Taiwan': 1077,\n",
       " 'Spain': 1061,\n",
       " 'Russia': 1047,\n",
       " 'Vietnam': 1047,\n",
       " 'Korea': 1034,\n",
       " 'Brazil': 905,\n",
       " 'Belgium': 702,\n",
       " 'U.S.A': 633,\n",
       " 'France': 600,\n",
       " 'Poland': 386,\n",
       " 'Australia': 378,\n",
       " 'Mexico': 357,\n",
       " 'Indonesia': 279,\n",
       " 'Italy': 228,\n",
       " 'Germany': 218,\n",
       " 'Panama': 160,\n",
       " 'Hong Kong': 133,\n",
       " 'Romania': 128,\n",
       " 'United Kingdom': 116,\n",
       " 'The Netherlands': 116,\n",
       " 'Thailand': 112,\n",
       " 'Colombia': 99,\n",
       " 'Hungary': 94,\n",
       " 'Ukraine': 91,\n",
       " 'Norway': 76,\n",
       " 'Jordan': 76,\n",
       " 'Chile': 67,\n",
       " 'India': 53,\n",
       " 'Afghanistan': 48,\n",
       " 'Iran': 45,\n",
       " 'Peru': 43,\n",
       " 'Canada': 41,\n",
       " 'Venezuela': 39,\n",
       " 'New Zealand': 29,\n",
       " 'Saudi Arabia': 20,\n",
       " 'Uruguay': 18,\n",
       " 'Portugal': 17,\n",
       " 'Bangladesh': 15,\n",
       " 'Singapore': 13,\n",
       " 'Egypt': 9,\n",
       " 'Malaysia': 6,\n",
       " 'Mongolia': 5,\n",
       " 'Nicaragua': 4,\n",
       " 'Cyprus': 3,\n",
       " 'Latvia': 2,\n",
       " 'Israel': 2,\n",
       " 'Sri Lanka': 1,\n",
       " 'Sweden': 1,\n",
       " 'Ecuador': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(CORPUS, METADATA, on=\"user_id\").groupby([\"location\"]).count()[\"user_id\"].sort_values(ascending=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
