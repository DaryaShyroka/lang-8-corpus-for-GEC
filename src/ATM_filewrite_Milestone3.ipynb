{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For week 3 Milestone\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "user_id = []\n",
    "doc_id = []\n",
    "original = []\n",
    "corrected = []\n",
    "file_row = []\n",
    "\n",
    "# read raw sentences from a preliminary corpus file. \n",
    "input_file = 'data/unannotated_sents.csv'\n",
    "f = open(input_file)\n",
    "lang8reader = csv.DictReader(f)\n",
    "for i, row in enumerate(lang8reader):\n",
    "    if np.abs(len(row['original']) - len(row['corrected'])) < 10 and '*' not in row['corrected'] and len(row['corrected'].split()) > 10:\n",
    "        file_row.append(i)                      # contains the row number of the input file \n",
    "        user_id.append(row['user_id'])\n",
    "        doc_id.append(row['doc_id'])\n",
    "        original.append(row['original'])\n",
    "        corrected.append(row['corrected'])\n",
    "f.close()\n",
    "\n",
    "f = open(input_file)\n",
    "f.readline()\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the length of the corrected sentences \n",
    "sort_index = sorted(range(len(corrected)), key=lambda k: len(corrected[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 550 longest sentences\n",
    "\n",
    "sentences_index_550 = sort_index[:-550]   \n",
    "sentences_index_550 = sentences_index_550[::-1]\n",
    "sentences_index_550 = sentences_index_550[:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row numbers of sentenes selected for annotations\n",
    "# row numbers refer to the row numbers in unannotated_sents.csv\n",
    "selected_for_annotations = []\n",
    "for idx in sentences_index_550:\n",
    "    selected_for_annotations.append(file_row[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_550 = []\n",
    "doc_550 = []\n",
    "original_550 = []\n",
    "corrected_550 = []\n",
    "\n",
    "for idx in sentences_index_550:\n",
    "    user_550.append(user_id[idx])\n",
    "    doc_550.append(doc_id[idx])\n",
    "    original_550.append(original[idx])\n",
    "    corrected_550.append(corrected[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552546,228529401510398295439370434580073286221,Utterson is really worried about Jekyll so he decided to visit Jekyll but he is not at home and at this moment Poole tells to the lawyer that Hyde has a key for the lavatory and that the servants had to obey to Hyde.,Utterson is really worried about Jekyll so he decided to visit Jekyll but he is not at home and at this moment Poole tells the lawyer that Hyde has a key for the lavatory and that the servants had to obey Hyde.\n",
      "\n",
      "1552546\n",
      "228529401510398295439370434580073286221\n",
      "Utterson is really worried about Jekyll so he decided to visit Jekyll but he is not at home and at this moment Poole tells to the lawyer that Hyde has a key for the lavatory and that the servants had to obey to Hyde.\n",
      "Utterson is really worried about Jekyll so he decided to visit Jekyll but he is not at home and at this moment Poole tells the lawyer that Hyde has a key for the lavatory and that the servants had to obey Hyde.\n",
      "\n",
      "\n",
      "337776,205802434485227789747646506865722854203,\"My English teachers in primary school or even in senior high didn't spend much time teaching us pronunciation.(Actually they simply reckon grammar the most important part in English learning, while I don't think so)\",\"My English teachers in primary school or even in high school didn't spend much time teaching us pronunciation.( they  thought that grammar is the most important part of English learning, while I don't think so)\"\n",
      "\n",
      "337776\n",
      "205802434485227789747646506865722854203\n",
      "My English teachers in primary school or even in senior high didn't spend much time teaching us pronunciation.(Actually they simply reckon grammar the most important part in English learning, while I don't think so)\n",
      "My English teachers in primary school or even in high school didn't spend much time teaching us pronunciation.( they  thought that grammar is the most important part of English learning, while I don't think so)\n",
      "\n",
      "\n",
      "337776,252898681647313682507037121299255887782,\"Compared to working hard to learn a language that I probably won't get any chance to use in the future, I prefer to playing games, which can give me a sense of excitement in a comparatively short time.\",\"Compared to working hard to learn a language that I probably won't get any chance to use in the future, I prefer playing games, which can give me a sense of excitement in a comparatively shorter amount of time.\"\n",
      "\n",
      "337776\n",
      "252898681647313682507037121299255887782\n",
      "Compared to working hard to learn a language that I probably won't get any chance to use in the future, I prefer to playing games, which can give me a sense of excitement in a comparatively short time.\n",
      "Compared to working hard to learn a language that I probably won't get any chance to use in the future, I prefer playing games, which can give me a sense of excitement in a comparatively shorter amount of time.\n",
      "\n",
      "\n",
      "905009,144034914011022270417791546071360889685,\"For ordinary customers, it is not easy to discern between a cold and the flu, therefore it is better to suggest drugs consisted of only acetaminophen or crude medicine materials especially during the flu season.\",\"For ordinary customers, it is not easy to tell the difference between a cold and the flu, so it is better to suggest drugs consisting only of acetaminophen or natural remedies, especially during the flu season.\"\n",
      "\n",
      "905009\n",
      "144034914011022270417791546071360889685\n",
      "For ordinary customers, it is not easy to discern between a cold and the flu, therefore it is better to suggest drugs consisted of only acetaminophen or crude medicine materials especially during the flu season.\n",
      "For ordinary customers, it is not easy to tell the difference between a cold and the flu, so it is better to suggest drugs consisting only of acetaminophen or natural remedies, especially during the flu season.\n",
      "\n",
      "\n",
      "892789,35619139520254737284944152836730851066,\"AS you know, I am currently discussing job applications with our university employment center staff but I wanted to ask if I could schedule an appointment to hear about the music industry and your take on it.\",\"As you know, I am currently discussing job applications with our university employment center staff, but I wanted to ask if I could schedule an appointment to hear about the music industry and your take on it.\"\n",
      "\n",
      "892789\n",
      "35619139520254737284944152836730851066\n",
      "AS you know, I am currently discussing job applications with our university employment center staff but I wanted to ask if I could schedule an appointment to hear about the music industry and your take on it.\n",
      "As you know, I am currently discussing job applications with our university employment center staff, but I wanted to ask if I could schedule an appointment to hear about the music industry and your take on it.\n",
      "\n",
      "\n",
      "1445993,203767320060102064743641603187170640942,\"In conclusion, I think that long-term traffic and pollution reductions would depend on education the public to use public transport more, and on governments using public money to construct and run efficient system.\",\"In conclusion, I think that long-term traffic and pollution reduction  depends on educating the public to use public transport more and on governments using public money to construct and run efficient systems.\"\n",
      "\n",
      "1445993\n",
      "203767320060102064743641603187170640942\n",
      "In conclusion, I think that long-term traffic and pollution reductions would depend on education the public to use public transport more, and on governments using public money to construct and run efficient system.\n",
      "In conclusion, I think that long-term traffic and pollution reduction  depends on educating the public to use public transport more and on governments using public money to construct and run efficient systems.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for i, idx in enumerate(sentences_index_550):\n",
    "    print(lines[file_row[idx]])\n",
    "    print(user_550[i])\n",
    "    print(doc_550[i])\n",
    "    print(original_550[i])\n",
    "    print(corrected_550[i])\n",
    "    print('\\n')\n",
    "    if i == 5:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv file \n",
    "output_file = 'data/AMT_Milestone3.csv'\n",
    "fout = open(output_file, 'w', encoding = 'utf-8')\n",
    "writer = csv.writer(fout, dialect = 'unix')\n",
    "writer.writerow(['orig', 'corr'])\n",
    "for i in range(550):\n",
    "    orig_sent = original_550[i]\n",
    "    corr_sent = corrected_550[i]\n",
    "    writer.writerow([orig_sent, corr_sent])\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv file \n",
    "output_file = 'data/AMT_Milestone3-metadata.csv'\n",
    "fout = open(output_file, 'w', encoding = 'utf-8')\n",
    "writer = csv.writer(fout, dialect = 'unix')\n",
    "writer.writerow(['row number in the file', 'doc id'])\n",
    "for i, idx in enumerate(sentences_index_550):\n",
    "    row_num = file_row[idx]\n",
    "    doc_id = doc_550[i]\n",
    "    writer.writerow([row_num+2, doc_id])\n",
    "    if i == 549:\n",
    "        break\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_index_550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the result file agree/disagree\n",
    "agree = []\n",
    "total_agree = []\n",
    "result_file = 'data/AMT_Milestone3_results.csv'\n",
    "f = open(result_file)\n",
    "results_reader = csv.DictReader(f)\n",
    "for i, row in enumerate(results_reader):\n",
    "    agree.append(row['agree'])  \n",
    "    total_agree.append(row['total agree'])\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the partial corpus (wihtout overlap from 2019 Lang 8 )\n",
    "\n",
    "file_row_final = []\n",
    "user_id_final = []\n",
    "doc_id_final = []\n",
    "original_final = []\n",
    "corrected_final = []\n",
    "global_agree = []\n",
    "global_total_agree = []\n",
    "f = open(input_file)\n",
    "lang8reader = csv.DictReader(f)\n",
    "for i, row in enumerate(lang8reader):\n",
    "    file_row_final.append(i)                      # contains the row number of the input file \n",
    "    user_id_final.append(row['user_id'])\n",
    "    doc_id_final.append(row['doc_id'])\n",
    "    original_final.append(row['original'])\n",
    "    corrected_final.append(row['corrected'])\n",
    "    if i in set(selected_for_annotations):\n",
    "        idx = file_row.index(i)\n",
    "        idx2 = sentences_index_550.index(idx)\n",
    "        global_agree.append(agree[idx2])\n",
    "        global_total_agree.append(total_agree[idx2])\n",
    "    else:\n",
    "        global_agree.append(None)\n",
    "        global_total_agree.append(None)    \n",
    "f.close()\n",
    "\n",
    "annotated_partial_corpus = 'data/annotated_sents.csv'\n",
    "fout = open(annotated_partial_corpus, 'w', encoding = 'utf-8')\n",
    "writer = csv.writer(fout, dialect = 'unix')\n",
    "writer.writerow(['user_id','doc_id','original','corrected','agree','total_agree'])\n",
    "for i in range(len(file_row_final)):\n",
    "    writer.writerow([user_id_final[i], doc_id_final[i], original_final[i], corrected_final[i], global_agree[i], global_total_agree[i]])\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
