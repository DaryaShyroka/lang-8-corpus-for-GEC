{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scraping instructions:\n",
    "\n",
    "    1. Scrape lang-8 user's metadata including user_id, profile, native language, friends list and __a list of journal_ids__.\n",
    "    (Note: example link to scrape will be https://lang-8.com/{user_id}/friends)\n",
    "    \n",
    "    2. Scrape parallel sentences with original and corrected sentences based on the user_id and journal_id pair.\n",
    "    (Note: example link to scrape will be https://lang-8.com/{user_id}/journals/{journal_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- POC:\n",
    "    1. Open scraper.ipynb and run all cells except the last two cells(those two should be run to scrape the whole corpus)\n",
    "    2. `scrape_metadata(\"./data/test_user_meta.txt\", set([test_user_id]), done_users_list=set(), total_count=1)` is going to scrape one user profile and stored into data/test_user_meta.txt file.\n",
    "    3. `scrape_parralel_sents(\"./data/test_user_meta.txt\", \"./data/test_paral_sents.txt\", total_count=1)` is going to scrape parallel sentences by reading user_id and journal_id pair and stored into data/test_paral_sents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step-by-step algorithm to store whole corpus\n",
    "    1. Collect all user metadata first and store data in the format below.\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    <user_id> 191274\n",
    "    ID: 191274\n",
    "    L points: 1,296,450\n",
    "    friends: []\n",
    "    documents: ['271352583976235145865979226599667907075', '329043475455030275589365621838457453059']\n",
    "    ```\n",
    "    2. Collect all friends and journals on all pages. (We only scrape one page for now just for simplicity.)\n",
    "    \n",
    "    3. Create a todo users list to maintain all users to be scraped and a done users list to maintain all users who have been already scraped. (Already implemented)\n",
    "    \n",
    "    4. Create a file loader to find user_id and doc_id pair from the scraped file from step 1. Use generator to avoid loading all users into memory. (Already implemented)\n",
    "    \n",
    "    5. After getting the (user_id, doc_id) pair, use this information to scrape journals which is written in English and store the original and corrected parallel sentences into a file in the format below. (Already implemented)\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    <user_id> 213725 , <doc_id> 256361636188698521397028321337896574465\n",
    "    org: I bought it, when we got my house built last month.\n",
    "    cor: ['I bought it when we got my house built last month.']\n",
    "    ```\n",
    "    *(Note: The reason we store a list in corrected sentences is because there might be several native speakers gave comments on the same sentences, so we scraped all of them as a reference.)*\n",
    "    \n",
    "    6. Create a todo journal list to maintain all journals to be scraped and a done journal list to maintain all journals which have been already scraped. (To be implemented)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
